\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
Modern Data Center Networks provide connectivity among several thousands of servers, hosting applications crucial for the business of customers all around the world. Likewise, next generation mobile and fixed networks are planned to replace specialized hardware with virtualized functions, running in general purpose devices in data center facilities. Heterogeneous traffic flows with different QoS demands share the same DCN infrastructure, requiring the adoption of sophisticated traffic control algorithms to accommodate their different needs. Usually the objective is to minimize the Flow Completion Time for short latency-constrained flows, while not loosing throughput for large bandwidth-demanding flows. Existing solutions leverage priority queues, transport protocols and schedulers in the network to approximate the performances of SRPT policy, which is proven to minimize the mean FCT. Some of them achieve near-optimal performances, however they either require the knowledge of flow sizes in advance --- implying undesired modifications to the application/transport layer --- or they assume the availability of many priority queues (PQ).

We investigated a network wide solution, that considers top-of-rack switches of a leaf-and-spine topology as a whole switch, sharing the queues. We argued that this method could augment the priority granularity, still enforcing only 2 PQ per switch interface. Indeed, the actual availability of priority queues in data centers' commodity switches is typically limited. We assumed to know only the overall probability distribution of flow sizes at data center level, but not their exact size at transport layer upon flow initiation. Thus, our solution appears promising and easy to implement with current technology.

We first introduced the necessary background. In particular, we revised size-aware and size-agnostic scheduling disciplines and systems that approximate size-agnostic policies such as LAS thanks to priority demotion. Then, in Chapter \ref{ch:sdframework} we presented our reference architecture, that exploits path redundancy among switches to transmit traffic of different priorities. As a consequence, the partition in priority levels directly affects the load balancing on the switching fabric. We referred to this main feature as spatial diversity. We proposed an analytical framework based on queuing theory, useful for deciding the optimal split of the flow size distribution in different priorities. Then, we proceeded in the assessment of system performances, when compared to state of the art solutions.

We discovered that our approach outperforms the others when FIFO servicing among flows is employed. In this case, scheduling both across priority queues and links mitigates the blockage of mice flows behind elephant flows. Conversely, we realized some unexpected impairments in case of PS queues. Long flows synchronize among each other during priority demotion and are slow to reach subsequent spines. Moreover,   
the way we handle flows attenuates the high variability property of the workload distributions observed by lowest priorities, which is fundamental for approaching optimal scheduling performances. 
%When the topology size grows, it is no more convenient to apply priority schemes in all switches for our system. 
Also, during time intervals with peak traffic arrival rate our system serves all flows initially on the same link, instead the random load balancing would distribute them on all available links, better absorbing the peak. We confirmed our results both in a numerical queuing simulator and in a TCP/IP network. 

Despite current congestion control protocols approximate Processor Sharing among flows, we think our research could be appealing in fully optical data center networks, where FIFO is common. To the best of our knowledge, there are no consolidated technologies to implement switch buffers in optics, without resorting to the conversion in electronic domain. Therefore, typically optical networks implement a circuit switching at flow level. The ever increasing bandwidth demand and especially the huge power consumption of data centers are pushing lot of interest towards optical solutions. 
%Currently, a widespread approach is to build hybrid networks, with optical and electrical interconnection fabrics working in parallel, with the optical network being devoted to elephant bandwidth-hungry flows.

To conclude, we illustrate further developments that we reserve for future work. One main limitation of our system is the handcrafted assumption of homogeneous and static flow size distribution. Actual data center networks may contain a variety of mixed workloads, that change along time. The solution to many optimization problems whenever it is needed the adaptation to the new load and distribution conditions, may be cumbersome and inefficient. Indeed, when the system parameters mismatch the actual environment conditions and especially if few priorities are used, the performances get worse. Thus, it is attractive the exploration automated self- 
%Since web applications pay also extra delays when traversing the Inter\textsl{}net to reach the end-user, latencies inside the data center must be within tight constraints, on the order of some milliseconds. A design principle driving the development of the system is its ease of deployment with current technologies without expensive hardware/software modifications.