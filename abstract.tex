\abstract
Major cloud computing providers agree on denoting the Flow Completion Time (FCT) as one of the primary goals to achieve in the design of a Data Center Network (DCN). This is motivated by the fact that latency affects the performance of interactive cloud computer services, such as web search, social networks and online applications, with a direct impact on the Quality of Experience as perceived by end users, hence ultimately on provider revenues.

Several approaches in literature rely on priority mechanisms at flow or packet level and schedulers in the network to reduce the average and tail FCT of latency-constrained flows. 
% albeit for many applications also tail latency is important. 
To this purpose, the Shortest Remaining Processing Time (SRPT) scheduling policy is proven to be optimal when the job size is known in advance. Unfortunately, this information is rarely available and would require expensive modifications to the protocol stack. Therefore, some proposals emulate a size-agnostic scheme that gives precedence to the flow with Least Attained Service (LAS), exploiting a finite number of priority queues in network switches. The effectiveness of these algorithms largely depends on the number of priority levels employed. However, DCNs are usually realized with low cost commodity devices, where only few queues per port - typically two - are vacant.

The contribution of this work is to investigate a design which exploits path abundance --- offered by DCN Fat-Tree topologies --- to intelligently route flows across the switching fabric, depending on their priority. The basic idea is to consider multiple switches as a whole switch, sharing the priority queues. Flows are scheduled both across priority queues and across multiple links, thus augmenting the priority granularity with spatial diversity. 
%In this load balancing and prioritization are jointly analyzed, in what multiple links together provide higher priority granularity than a port alone. ... with a scope limited to single bottleneck interfaces, the problem can be 
In short, flow scheduling is addressed at DC-wide level, considering the queues of different equal cost links together. Hence, unlike aforementioned techniques, load balancing and prioritization are jointly analyzed, since load balancing directly depend on flow priorities. This approach seems appealing for its very limited complexity. 

First, an analytical model is provided for the setting of optimal parameters. Then, it will be shown through a numerical fluid-model simulator that the proposed strategy indeed is helpful when the flow serving policy is FIFO. Conversely, few unexpected impairments under Processor Sharing discipline have been discovered. They are deeply analyzed, in relationship to the size of the topology and to the number of priority queues per interface. Finally, a broad spectrum of packet level simulations in a real data center topology have been conducted with OMNeT++ discrete-event simulator, in order to validate the FCT trends obtained in previous steps. 
%, but that long flows can be excessively penalized. .....
%We were able to show the same FCT trends and provide few additional insights . At the same time, we were able to identify downsides of this methodology and provide a possible strategy to overcome them.